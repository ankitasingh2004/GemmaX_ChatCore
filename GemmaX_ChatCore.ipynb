{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 318821,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 269005,
          "modelId": 279036
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **GemmaX ChatCore**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Xa2ZlM5mLBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A multimodal AI chat application built using Gemma 3 models, supporting\n",
        "cloud inference, vision analysis, local JAX inference, multi-turn chat, and function calling."
      ],
      "metadata": {
        "id": "Y1PrG6Q7qPP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Setup & Installation**"
      ],
      "metadata": {
        "id": "MZaUI-dkqlyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-genai==1.45.0"
      ],
      "metadata": {
        "id": "sm-iw7RVc0ER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Initialize Google GenAI Client (Cloud Inference)**"
      ],
      "metadata": {
        "id": "i5ovly3lq1iT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from IPython.display import HTML, Markdown, display"
      ],
      "metadata": {
        "id": "0u3ocjadV0hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai.types import Part\n",
        "\n",
        "client = genai.Client(api_key=userdata.get('gen-ai-key'))\n",
        "\n",
        "MODEL_ID = \"models/gemma-3-27b-it\""
      ],
      "metadata": {
        "id": "h6RN3DLpr4dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\"Who is Kalpana Chawla?\"],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "DwMh5k2paaQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Vision + Text (Multimodal Inference Using Gemma)**"
      ],
      "metadata": {
        "id": "a0nlue8LaLm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('/content/Kalpana Chawla.jpg', width=400, height=400)"
      ],
      "metadata": {
        "id": "04ZP0e0GV0e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/Kalpana Chawla.jpg\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
        "        \"What is this image about?\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "Q6M7stkzWFrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image('/content/Temple.jpg',width = 400, height = 400)"
      ],
      "metadata": {
        "id": "MgEwaptLgNUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "with open(\"/content/unnamed.jpg\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
        "        \"What is this image about and where is this place located? Tell something about the timing of this photo\",\n",
        "    ],\n",
        ")\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "rf5H6vRJk4Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Setup Local Gemma (JAX Backend)**"
      ],
      "metadata": {
        "id": "Ifz7WbAZa4hW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "kagglehub.login()\n",
        "\n"
      ],
      "metadata": {
        "id": "TmDzTUjL_XQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip uninstall -y -q keras-hub\n",
        "!pip install -q -U keras-hub\n",
        "!pip install  -q -U keras"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T12:59:31.582035Z",
          "iopub.execute_input": "2025-04-04T12:59:31.582352Z",
          "iopub.status.idle": "2025-04-04T12:59:39.772573Z",
          "shell.execute_reply.started": "2025-04-04T12:59:31.582328Z",
          "shell.execute_reply": "2025-04-04T12:59:39.771593Z"
        },
        "id": "kszoEYSP-gKo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1. Configure JAX GPU Backend**"
      ],
      "metadata": {
        "id": "ZDtiFSpX-gKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T12:59:39.774067Z",
          "iopub.execute_input": "2025-04-04T12:59:39.774425Z",
          "iopub.status.idle": "2025-04-04T12:59:39.778399Z",
          "shell.execute_reply.started": "2025-04-04T12:59:39.774392Z",
          "shell.execute_reply": "2025-04-04T12:59:39.777454Z"
        },
        "id": "dOqaAnDY-gKo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "v9y4uL0W-YXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Import Dependencies**"
      ],
      "metadata": {
        "id": "UeQBC0WYrqKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import re\n",
        "\n",
        "import keras\n",
        "import keras_hub\n",
        "\n",
        "import numpy as np\n",
        "import PIL\n",
        "import requests\n",
        "\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T12:59:39.779853Z",
          "iopub.execute_input": "2025-04-04T12:59:39.780145Z",
          "iopub.status.idle": "2025-04-04T12:59:43.498926Z",
          "shell.execute_reply.started": "2025-04-04T12:59:39.780114Z",
          "shell.execute_reply": "2025-04-04T12:59:43.498229Z"
        },
        "id": "ufsJml6G-gKo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Utility Function for Formatting Model Output**"
      ],
      "metadata": {
        "id": "ugKUOuKZ-gKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_formatted_output(raw_text, only_model_result=False):\n",
        "\n",
        "    separator = '<end_of_turn>\\n<start_of_turn>model'\n",
        "\n",
        "    if separator not in raw_text:\n",
        "        print(\"Warning: Input text does not contain the expected separator.\")\n",
        "        print(\"Displaying raw text without formatting:\\n---\")\n",
        "        print(raw_text)\n",
        "        print(\"---\")\n",
        "        return\n",
        "\n",
        "    parts = raw_text.split(separator, 1)\n",
        "\n",
        "    if len(parts) != 2:\n",
        "         print(f\"Warning: Could not properly split the text with separator '{separator}'.\")\n",
        "         print(\"Displaying raw text without formatting:\\n---\")\n",
        "         print(raw_text)\n",
        "         print(\"---\")\n",
        "         return\n",
        "\n",
        "    user_prompt = parts[0].replace('<start_of_turn>user', '').strip()\n",
        "\n",
        "    model_response_raw = parts[1].replace('<end_of_turn>', '').strip()\n",
        "\n",
        "    model_response_formatted = re.sub(r'(?<!\\n)\\n(?!\\n)', r'<br>\\n', model_response_raw)\n",
        "\n",
        "    formatted_markdown = f\"\"\"**User:**\n",
        "\n",
        "> {user_prompt}\n",
        "\n",
        "---\n",
        "\n",
        "**Model:**\n",
        "\n",
        "{model_response_formatted}\n",
        "\"\"\"\n",
        "    if only_model_result:\n",
        "        return display(Markdown(model_response_formatted))\n",
        "    else:\n",
        "      display(Markdown(formatted_markdown))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T12:59:43.506552Z",
          "iopub.execute_input": "2025-04-04T12:59:43.506846Z",
          "iopub.status.idle": "2025-04-04T12:59:43.522974Z",
          "shell.execute_reply.started": "2025-04-04T12:59:43.506817Z",
          "shell.execute_reply": "2025-04-04T12:59:43.522458Z"
        },
        "id": "DeEOivjr-gKo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Load Gemma Model Locally**"
      ],
      "metadata": {
        "id": "GZ6RDUym-gKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm = keras_hub.models.Gemma3CausalLM.from_preset(\n",
        "    \"gemma3_instruct_4b\",\n",
        "    dtype=\"bfloat16\",\n",
        ")\n",
        "\n",
        "gemma_lm.summary()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T13:01:23.941805Z",
          "iopub.execute_input": "2025-04-04T13:01:23.942181Z",
          "iopub.status.idle": "2025-04-04T13:02:27.100470Z",
          "shell.execute_reply.started": "2025-04-04T13:01:23.942151Z",
          "shell.execute_reply": "2025-04-04T13:02:27.099731Z"
        },
        "id": "EVTtBS01-gKp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm.preprocessor.max_images_per_prompt = 2\n",
        "gemma_lm.preprocessor.sequence_length = 768"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T13:02:27.101458Z",
          "iopub.execute_input": "2025-04-04T13:02:27.101698Z",
          "iopub.status.idle": "2025-04-04T13:02:27.105189Z",
          "shell.execute_reply.started": "2025-04-04T13:02:27.101664Z",
          "shell.execute_reply": "2025-04-04T13:02:27.104530Z"
        },
        "id": "Q4ZmORQO-gKp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE = \"\"\"<start_of_turn>user\n",
        "{question}\n",
        "<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T13:02:27.106488Z",
          "iopub.execute_input": "2025-04-04T13:02:27.106790Z",
          "iopub.status.idle": "2025-04-04T13:02:27.125463Z",
          "shell.execute_reply.started": "2025-04-04T13:02:27.106760Z",
          "shell.execute_reply": "2025-04-04T13:02:27.124702Z"
        },
        "id": "rmBEXkX7-gKp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Multi-turn Chat Application**\n",
        "\n"
      ],
      "metadata": {
        "id": "Ke3n8D0v-gKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_turn_conversation(gemma_lm, initial_prompt):\n",
        "    conversation_history = initial_prompt\n",
        "    while True:\n",
        "        output = gemma_lm.generate(conversation_history)\n",
        "        display_formatted_output(output)\n",
        "\n",
        "        next_prompt = input(\"Your next question/input (or type 'exit' to end): \")\n",
        "        if next_prompt.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "        conversation_history += f\"\\n<end_of_turn>\\n<start_of_turn>user\\n{next_prompt}\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "initial_prompt = \"\"\"<start_of_turn>user\n",
        "Hey, what's up.\n",
        "<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "multi_turn_conversation(gemma_lm, initial_prompt)\n"
      ],
      "metadata": {
        "id": "Q_kY8rfvhGN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_turn_conversation(gemma_lm, initial_prompt)"
      ],
      "metadata": {
        "trusted": true,
        "id": "rjVW-URT-gKq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Function Calling Setup**"
      ],
      "metadata": {
        "id": "ITgmpRsrpsw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install CurrencyConverter\n",
        "from currency_converter import CurrencyConverter\n",
        "import datetime\n",
        "cc_object = CurrencyConverter()"
      ],
      "metadata": {
        "id": "lztJiDlVwS-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_current_datetime():\n",
        "  now = datetime.datetime.now()\n",
        "  current_date = now.strftime(\"%Y-%m-%d\")\n",
        "  current_time = now.strftime(\"%H:%M:%S\")\n",
        "  return f\"Current Date: {current_date}, Current Time: {current_time}\"\n",
        "\n",
        "\n",
        "def currency_converter(amount, source_currency, destinatoon_currency):\n",
        "  print(f\"(py function) fetching the {source_currency} to {destinatoon_currency} Rate\")\n",
        "  ans = cc_object.convert(amount, source_currency, destinatoon_currency)\n",
        "  ans = round(ans, 4)\n",
        "  return ans\n"
      ],
      "metadata": {
        "id": "AW4g7pU50FWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Function Calling Prompt Definition**"
      ],
      "metadata": {
        "id": "SEVPXv59tX-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fun_prompt = \"\"\"\n",
        "You have access to the following functions. If you choose to invoke any of them,\n",
        "you MUST use the exact format:\n",
        "\n",
        "[function_name(param1=value1, param2=value2, ...)]\n",
        "\n",
        "You SHOULD NOT include any other text in the response if you are calling a function.\n",
        "\n",
        "Available functions:\n",
        "\n",
        "[\n",
        "  {{\n",
        "    \"name\": \"get_current_datetime\",\n",
        "    \"description\": \"Returns the current date and time.\",\n",
        "    \"parameters\": {{\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {{}},\n",
        "      \"required\": []\n",
        "    }}\n",
        "  }},\n",
        "  {{\n",
        "    \"name\": \"currency_converter\",\n",
        "    \"description\": \"Converts a specified amount from one currency to another.\",\n",
        "    \"parameters\": {{\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {{\n",
        "        \"amount\": {{\n",
        "          \"type\": \"number\",\n",
        "          \"description\": \"The amount of money to convert.\"\n",
        "        }},\n",
        "        \"source_currency\": {{\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"The currency code of the original currency (e.g., 'USD').\"\n",
        "        }},\n",
        "        \"destinatoon_currency\": {{\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"The currency code to convert to (e.g., 'EUR').\"\n",
        "        }}\n",
        "      }},\n",
        "      \"required\": [\"amount\", \"source_currency\", \"destinatoon_currency\"]\n",
        "    }}\n",
        "  }}\n",
        "]\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "tfflJ1ENx4DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Execute Function Call with Gemma**"
      ],
      "metadata": {
        "id": "7IotOyDdthY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"I want to convert 20000 Indian rupee to US Dollar.\"\n",
        "\n",
        "full_prompt = f\"{fun_prompt}\\n\\nNow answer the following question:\\n{user_question}\"\n",
        "\n",
        "full_prompt = PROMPT_TEMPLATE.format(question=full_prompt)\n",
        "\n",
        "output = gemma_lm.generate(full_prompt)\n",
        "\n",
        "separator = '<end_of_turn>\\n<start_of_turn>model'\n",
        "parts = output.split(separator)\n",
        "response_fun = parts[1].replace('<end_of_turn>', '').strip()\n",
        "function_result = eval(response_fun)"
      ],
      "metadata": {
        "id": "ZHNi7f80x7Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "function_result"
      ],
      "metadata": {
        "id": "jBwP2ulERvd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. Convert Function Output to Natural Language**"
      ],
      "metadata": {
        "id": "1StuIroL3y0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "followup_prompt = f\"\"\"\n",
        "The function call `{response_fun}` was executed.\n",
        "The result is: {function_result}\n",
        "\n",
        "Write a natural, user-friendly response explaining this result. Also, ask a relevant follow-up question to be helpful or Suggest Related Information. Keep it short and skip any introductory phrases.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "followup_prompt = PROMPT_TEMPLATE.format(question=followup_prompt)\n",
        "final_response = gemma_lm.generate(followup_prompt)\n",
        "\n",
        "display_formatted_output(final_response, only_model_result=True)"
      ],
      "metadata": {
        "id": "zfhSK6_Jw_cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qg9_ScJO4HrA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}